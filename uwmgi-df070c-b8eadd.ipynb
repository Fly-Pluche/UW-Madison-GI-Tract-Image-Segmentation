{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q ../input/pytorch-segmentation-models-lib/pretrainedmodels-0.7.4/pretrainedmodels-0.7.4\n!pip install -q ../input/pytorch-segmentation-models-lib/efficientnet_pytorch-0.6.3/efficientnet_pytorch-0.6.3\n!pip install -q ../input/pytorch-segmentation-models-lib/timm-0.4.12-py3-none-any.whl\n!pip install -q ../input/pytorch-segmentation-models-lib/segmentation_models_pytorch-0.2.0-py3-none-any.whl\n!mkdir -p /root/.cache/torch/hub/checkpoints\n# !cp ../input/effb7-pth/efficientnet-b7-dcc49843.pth /root/.cache/torch/hub/checkpoints/efficientnet-b7-dcc49843.pth","metadata":{"execution":{"iopub.status.busy":"2022-07-14T14:25:45.422161Z","iopub.execute_input":"2022-07-14T14:25:45.422819Z","iopub.status.idle":"2022-07-14T14:25:57.520876Z","shell.execute_reply.started":"2022-07-14T14:25:45.422777Z","shell.execute_reply":"2022-07-14T14:25:57.515900Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nimport numpy as np\nimport pandas as pd\nimport cv2\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport segmentation_models_pytorch as smp\n\nfrom glob import glob","metadata":{"execution":{"iopub.status.busy":"2022-07-14T14:26:03.630527Z","iopub.execute_input":"2022-07-14T14:26:03.630905Z","iopub.status.idle":"2022-07-14T14:26:14.355130Z","shell.execute_reply.started":"2022-07-14T14:26:03.630862Z","shell.execute_reply":"2022-07-14T14:26:14.354218Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def get_class_names(df):\n    labels = df['class']\n    return labels.unique()\n\ndef make_test_augmenter(conf):\n    crop_size = round(conf.image_size*conf.crop_size)\n    return  A.Compose([\n        A.CenterCrop(height=crop_size, width=crop_size),\n        ToTensorV2(transpose_mask=True)\n    ])\n\ndef get_id(filename):\n    # e.g. filename: case123_day20/scans/slice_0001_266_266_1.50_1.50.png\n    # id: case123_day20_slice_0001\n    tokens = filename.split('/')\n    return tokens[-3] + '_' + '_'.join(tokens[-1].split('_')[:2])","metadata":{"execution":{"iopub.status.busy":"2022-07-14T14:26:14.358878Z","iopub.execute_input":"2022-07-14T14:26:14.359483Z","iopub.status.idle":"2022-07-14T14:26:14.366302Z","shell.execute_reply.started":"2022-07-14T14:26:14.359454Z","shell.execute_reply":"2022-07-14T14:26:14.365626Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import torch.utils.data as data\nclass VisionDataset(data.Dataset):\n    def __init__(\n            self, df, conf, input_dir, imgs_dir,\n            class_names, transform, is_test=False, subset=100):\n        self.conf = conf\n        self.transform = transform\n        self.is_test = is_test\n        self.CLAHE =cv2.createCLAHE(clipLimit=2.0,tileGridSize=(8,8))\n        \n        if subset != 100:\n            assert subset < 100\n            # train and validate on subsets\n            num_rows = df.shape[0]*subset//100\n            df = df.iloc[:num_rows]\n\n        files = df['img_files']\n        self.files = [os.path.join(input_dir, imgs_dir, f) for f in files]\n        self.masks = [f.replace('train', 'masks') for f in files]\n\n    def resize(self, img, interp):\n        return  cv2.resize(\n            img, (self.conf.image_size, self.conf.image_size), interpolation=interp)\n\n    def load_slice(self, img_file, diff):\n        slice_num = os.path.basename(img_file).split('_')[1]\n        filename = (\n            img_file.replace(\n                'slice_' + slice_num,\n                'slice_' + str(int(slice_num) + diff).zfill(4)))\n        if os.path.exists(filename):\n#             clahe = cv2.createCLAHE(clipLimit=2, tileGridSize=(8, 8))\n            return self.CLAHE.apply(cv2.imread(filename, cv2.IMREAD_UNCHANGED))\n        return None\n\n    def __getitem__(self, index):\n        conf = self.conf\n        img_file = self.files[index]\n        # read 5 slices into one image\n        imgs = [self.load_slice(img_file, i) for i in range(-2, 3)]\n        if imgs[3] is None:\n            imgs[3] = imgs[2]\n        if imgs[4] is None:\n            imgs[4] = imgs[3]\n        if imgs[1] is None:\n            imgs[1] = imgs[2]\n        if imgs[0] is None:\n            imgs[0] = imgs[1]\n        img = np.stack(imgs, axis=2)\n\n        img = img.astype(np.float32)\n        max_val = img.max()\n        if max_val != 0:\n            img /= max_val\n        img = self.resize(img, cv2.INTER_AREA)\n\n        if self.is_test:\n            msk = 0\n            result = self.transform(image=img)\n            img = result['image']\n        else:\n            # read mask\n            msk_file = self.masks[index]\n            msk = cv2.imread(msk_file, cv2.IMREAD_UNCHANGED)\n            msk = self.resize(msk, cv2.INTER_NEAREST)\n            msk = msk.astype(np.float32)\n            result = self.transform(image=img, mask=msk)\n            img, msk = result['image'], result['mask']\n        return img, msk\n\n    def __len__(self):\n        return len(self.files)","metadata":{"execution":{"iopub.status.busy":"2022-07-14T14:26:14.367563Z","iopub.execute_input":"2022-07-14T14:26:14.368027Z","iopub.status.idle":"2022-07-14T14:26:14.586342Z","shell.execute_reply.started":"2022-07-14T14:26:14.367988Z","shell.execute_reply":"2022-07-14T14:26:14.585445Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\nimport segmentation_models_pytorch as smp\n\nclass ModelWrapper(nn.Module):\n\n    def __init__(self, conf, num_classes):\n        super().__init__()\n        if conf.arch == 'FPN':\n            arch = smp.FPN\n        elif conf.arch == 'Unet':\n            arch = smp.Unet\n        elif conf.arch == 'DeepLabV3':\n            arch = smp.DeepLabV3\n        else:\n            assert 0, f'Unknown architecture {conf.arch}'\n\n        weights = 'imagenet' if conf.pretrained else None\n        self.model = arch(\n            encoder_name=conf.backbone, encoder_weights=weights, in_channels=5,\n            classes=num_classes, activation=None)\n\n    def forward(self, x):\n        x = self.model(x)\n        return  x\n","metadata":{"execution":{"iopub.status.busy":"2022-07-14T14:26:14.590718Z","iopub.execute_input":"2022-07-14T14:26:14.591199Z","iopub.status.idle":"2022-07-14T14:26:14.602244Z","shell.execute_reply.started":"2022-07-14T14:26:14.591156Z","shell.execute_reply":"2022-07-14T14:26:14.601550Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from scipy.ndimage.morphology import binary_dilation\ndef drop_small_mask(mask, th):\n    res = np.zeros_like(mask)\n    ret, labels = cv2.connectedComponents(mask.astype(np.uint8), connectivity=4)\n    for i in range(1, ret):\n        if np.sum(labels==i) > th:\n            res += (labels==i)\n    return res  ","metadata":{"execution":{"iopub.status.busy":"2022-07-14T14:26:14.605188Z","iopub.execute_input":"2022-07-14T14:26:14.605524Z","iopub.status.idle":"2022-07-14T14:26:14.612262Z","shell.execute_reply.started":"2022-07-14T14:26:14.605487Z","shell.execute_reply":"2022-07-14T14:26:14.611361Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class Config():\n    # FPN, Unet or DeepLab\n    arch = 'FPN'\n    backbone = 'efficientnet-b7'\n    pretrained = True\n\n    # resize images to this size on the fly\n    image_size = 512\n    # crop to this fraction of image_size\n    crop_size = 1.0\n\n    # optimizer settings\n    optim = 'adam'\n    lr = 0.001\n    weight_decay = 0.01\n    batch_size = 48\n\n    # scheduler settings\n    gamma = 0.96\n\n    # data augmentation\n    aug_prob = 0.4\n    strong_aug = True\n    max_cutout = 0\n    \nconf = Config()\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2022-07-14T14:26:14.613890Z","iopub.execute_input":"2022-07-14T14:26:14.614410Z","iopub.status.idle":"2022-07-14T14:26:14.681731Z","shell.execute_reply.started":"2022-07-14T14:26:14.614374Z","shell.execute_reply":"2022-07-14T14:26:14.680771Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def create_test_loader(conf, input_dir, class_names):\n    test_aug = make_test_augmenter(conf)\n    test_df = pd.DataFrame()\n    img_files = []\n    img_dir = 'test'\n    subdir = ''\n    while len(img_files) == 0 and len(subdir) < 10:\n        img_files = sorted(glob(f'{input_dir}/{img_dir}/{subdir}*.png'))\n        subdir += '*/'\n        if len(subdir) > 10:\n            return None\n    # delete common prefix from paths\n    if len(img_files) == 0:\n        img_dir = 'train'\n        subdir = ''\n        while len(img_files) == 0 and len(subdir) < 10:\n            img_files = sorted(glob(f'{input_dir}/{img_dir}/{subdir}*.png'))\n            subdir += '*/'\n            if len(subdir) > 10:\n                return None      \n        img_files = img_files[:1000]\n    img_files = [f.replace(f'{input_dir}/{img_dir}/', '') for f in img_files]\n\n    test_df['img_files'] = img_files\n    test_dataset = VisionDataset(\n        test_df, conf, input_dir, img_dir,\n        class_names, test_aug, is_test=True)\n    print(f'{len(test_dataset)} examples in test set')\n    loader = data.DataLoader(\n        test_dataset, batch_size=conf.batch_size, shuffle=False,\n        num_workers=2, pin_memory=False)\n    return loader, test_df","metadata":{"execution":{"iopub.status.busy":"2022-07-14T14:26:14.683213Z","iopub.execute_input":"2022-07-14T14:26:14.683729Z","iopub.status.idle":"2022-07-14T14:26:14.701460Z","shell.execute_reply.started":"2022-07-14T14:26:14.683644Z","shell.execute_reply":"2022-07-14T14:26:14.700807Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def create_model(conf, model_dir, num_classes):\n    checkpoint = torch.load(model_dir, map_location=device)['model']\n    pretrained_dict = {k.replace('module.', '') : v for k, v in checkpoint.items()}\n    conf.pretrained = False  \n    model = ModelWrapper(conf, num_classes)\n    model = model.to(device)\n    model.load_state_dict(pretrained_dict)\n    return model\n\n\n# def create_model(conf, model_dir, num_classes):\n# #     checkpoint = torch.load(model_dir, map_location=device)['model']\n# #     pretrained_dict = {k.replace('module.', '') : v for k, v in checkpoint.items()}\n#     conf.pretrained = False  \n#     model = ModelWrapper(conf, num_classes)\n#     model = model.to(device)\n# #     model.load_state_dict(pretrained_dict)\n#     return model","metadata":{"execution":{"iopub.status.busy":"2022-07-14T14:26:14.702783Z","iopub.execute_input":"2022-07-14T14:26:14.703098Z","iopub.status.idle":"2022-07-14T14:26:14.714032Z","shell.execute_reply.started":"2022-07-14T14:26:14.703060Z","shell.execute_reply":"2022-07-14T14:26:14.713161Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def rle_encode(img):\n    '''\n    this function is adapted from\n    https://www.kaggle.com/code/stainsby/fast-tested-rle/notebook\n    img: numpy array, 1 - mask, 0 - background\n    Returns run length as string formated\n    '''\n    pixels = img.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)\n\ndef get_img_shape(filename):\n    basename = os.path.basename(filename)\n    tokens = basename.split('_')\n    height, width = int(tokens[3]), int(tokens[2])\n    return (height, width)\n\ndef pad_mask(conf, mask):\n    # pad image to conf.image_size\n    padded = np.zeros((conf.image_size, conf.image_size), dtype=mask.dtype)\n    dh = conf.image_size - mask.shape[0]\n    dw = conf.image_size - mask.shape[1]\n\n    top = dh//2\n    left = dw//2\n    padded[top:top + mask.shape[0], left:left + mask.shape[1]] = mask\n    return padded\n\ndef resize_mask(mask, height, width):\n    return cv2.resize(mask, (width, height), interpolation=cv2.INTER_NEAREST)","metadata":{"execution":{"iopub.status.busy":"2022-07-14T14:26:14.716549Z","iopub.execute_input":"2022-07-14T14:26:14.717343Z","iopub.status.idle":"2022-07-14T14:26:14.729117Z","shell.execute_reply.started":"2022-07-14T14:26:14.717278Z","shell.execute_reply":"2022-07-14T14:26:14.728423Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"import cv2\nimport numpy as np\n\n'''\n图像说明：\n图像为二值化图像，255白色为目标物，0黑色为背景\n要填充白色目标物中的黑色空洞\n'''\n\n\ndef FillHole(im_in):\n    # 复制 im_in 图像\n\n    im_floodfill = im_in.copy()\n\n    # Mask 用于 floodFill，官方要求长宽+2\n    h, w = im_in.shape[:2]\n    mask = np.zeros((h + 2, w + 2), np.uint8)\n\n    seedPoint=(0,0)\n    cv2.floodFill(im_floodfill, mask, seedPoint, 1)\n\n    # 得到im_floodfill的逆im_floodfill_inv\n    im_floodfill_inv = cv2.bitwise_not(im_floodfill)\n\n    # 把im_in、im_floodfill_inv这两幅图像结合起来得到前景\n    im_out = im_in * im_floodfill_inv\n               \n    return im_out","metadata":{"execution":{"iopub.status.busy":"2022-07-14T14:29:26.781552Z","iopub.execute_input":"2022-07-14T14:29:26.782238Z","iopub.status.idle":"2022-07-14T14:29:26.789642Z","shell.execute_reply.started":"2022-07-14T14:29:26.782198Z","shell.execute_reply":"2022-07-14T14:29:26.788906Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"import PIL\nimport torchvision\nimport torchvision.transforms.functional as F\ndef run(input_dir, ckpt_paths, thresh):\n    meta_file = os.path.join(input_dir, 'train.csv')\n    train_df = pd.read_csv(meta_file, dtype=str)\n    class_names = np.array(get_class_names(train_df))\n    num_classes = len(class_names)\n    loader, df = create_test_loader(conf, input_dir, class_names)\n    img_files = df['img_files']\n    subm = pd.read_csv(f'{input_dir}/sample_submission.csv')\n    del subm['predicted']\n    ids = []\n    classes = []\n    masks = []\n    img_idx = 0\n    sigmoid = nn.Sigmoid()\n#     model.eval()\n\n    with torch.no_grad():\n        for images, _ in loader:\n            images = images.to(device)\n            size = images.size()\n            masks_tta = np.zeros((size[0], 3, size[2], size[3]), dtype=np.float32)\n            for sub_ckpt_path in ckpt_paths:\n                model = create_model(conf, sub_ckpt_path, num_classes)\n                model.eval()\n                outputs = model(images)\n                preds = sigmoid(outputs).cpu().numpy()\n                masks_tta += preds\n                flips = [[-1]]\n                for f in flips:\n                    images_f = torch.flip(images, f)\n                    outputs = model(images_f)\n                    outputs = torch.flip(outputs, f)\n                    preds = sigmoid(outputs).cpu().numpy()\n                    masks_tta += preds\n                for degree in [25, -25]:\n                    images_d = torchvision.transforms.RandomRotation(degrees=(degree, degree), expand=False, center=(size[2]//2, size[3]//2))(images)\n                    outputs = model(images_d)\n                    outputs = torchvision.transforms.RandomRotation(degrees=(-degree, -degree), expand=False, center=(size[2]//2, size[3]//2))(outputs)\n                    preds = sigmoid(outputs).cpu().numpy()\n                    masks_tta += preds\n            masks_tta /= 4*len(ckpt_paths)\n            masks_tta[masks_tta >= thresh] = 1\n            masks_tta[masks_tta < thresh] = 0\n            mask_list=[]\n#             for mask in masks_tta:\n#                 im_out=FillHole(mask)\n#                 mask_list.append(im_out)\n#             masks_tta=np.stack(mask_list)\n#             print(masks_tta.shape)\n            \n            for pred in masks_tta:\n                img_file = img_files[img_idx]\n                img_idx += 1\n                img_id = get_id(img_file)\n                height, width = get_img_shape(img_file)\n                for class_id, class_name in enumerate(class_names):\n                    mask = pred[class_id]\n#                     mask = cv2.GaussianBlur(mask, (5,5), sigmaX=2)\n                    #mask[mask >= thresh] = 1\n                    #mask[mask < thresh] = 0\n                    mask = drop_small_mask(mask, 128)\n                    mask = FillHole(mask)\n                    mask = pad_mask(conf, mask)\n                    mask = resize_mask(mask, height, width)\n                    enc_mask = '' if mask.sum() == 0 else rle_encode(mask)\n                    ids.append(img_id)\n                    classes.append(class_name)\n                    masks.append(enc_mask)\n\n    pred_df = pd.DataFrame({'id': ids, 'class': classes, 'predicted': masks})\n    if pred_df.shape[0] > 0:\n        # sort according to the given order and save to a csv file\n        subm = subm.merge(pred_df, on=['id', 'class'])\n\n        subm.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-07-14T15:12:52.043446Z","iopub.execute_input":"2022-07-14T15:12:52.043754Z","iopub.status.idle":"2022-07-14T15:12:52.062944Z","shell.execute_reply.started":"2022-07-14T15:12:52.043714Z","shell.execute_reply":"2022-07-14T15:12:52.062169Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"test_thresh = 0.35\nckpt_paths ='../input/checkpoint/*.pth'\nckpt_paths_list=glob(ckpt_paths)\nprint(ckpt_paths_list)\nrun('../input/uw-madison-gi-tract-image-segmentation', ckpt_paths_list, test_thresh)","metadata":{"execution":{"iopub.status.busy":"2022-07-14T15:12:53.167343Z","iopub.execute_input":"2022-07-14T15:12:53.168438Z","iopub.status.idle":"2022-07-14T15:17:35.766638Z","shell.execute_reply.started":"2022-07-14T15:12:53.168380Z","shell.execute_reply":"2022-07-14T15:17:35.765582Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-07-14T14:27:47.950239Z","iopub.execute_input":"2022-07-14T14:27:47.950562Z","iopub.status.idle":"2022-07-14T14:27:47.956173Z","shell.execute_reply.started":"2022-07-14T14:27:47.950523Z","shell.execute_reply":"2022-07-14T14:27:47.955464Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"FillHole(mask).shape","metadata":{"execution":{"iopub.status.busy":"2022-07-14T14:29:28.652272Z","iopub.execute_input":"2022-07-14T14:29:28.652912Z","iopub.status.idle":"2022-07-14T14:29:28.672068Z","shell.execute_reply.started":"2022-07-14T14:29:28.652872Z","shell.execute_reply":"2022-07-14T14:29:28.671250Z"},"trusted":true},"execution_count":19,"outputs":[]}]}