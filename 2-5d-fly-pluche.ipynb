{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2f9795c",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2022-06-30T14:07:58.120782Z",
     "iopub.status.busy": "2022-06-30T14:07:58.120004Z",
     "iopub.status.idle": "2022-06-30T14:09:56.794474Z",
     "shell.execute_reply": "2022-06-30T14:09:56.793346Z"
    },
    "papermill": {
     "duration": 118.683442,
     "end_time": "2022-06-30T14:09:56.797076",
     "exception": false,
     "start_time": "2022-06-30T14:07:58.113634",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q ../input/wheels/pretrainedmodels-0.7.4-py3-none-any.whl\n",
    "!pip install -q ../input/wheels/efficientnet_pytorch-0.6.3-py3-none-any.whl\n",
    "!pip install -q ../input/wheels/timm-0.4.12-py3-none-any.whl\n",
    "!pip install -q ../input/wheels/segmentation_models_pytorch-0.2.1-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09d928e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-30T14:09:56.808021Z",
     "iopub.status.busy": "2022-06-30T14:09:56.807627Z",
     "iopub.status.idle": "2022-06-30T14:10:07.111428Z",
     "shell.execute_reply": "2022-06-30T14:10:07.110600Z"
    },
    "papermill": {
     "duration": 10.311982,
     "end_time": "2022-06-30T14:10:07.113763",
     "exception": false,
     "start_time": "2022-06-30T14:09:56.801781",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import segmentation_models_pytorch as smp\n",
    "import  random\n",
    "\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1dc67bae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-30T14:10:07.124454Z",
     "iopub.status.busy": "2022-06-30T14:10:07.123948Z",
     "iopub.status.idle": "2022-06-30T14:10:07.130367Z",
     "shell.execute_reply": "2022-06-30T14:10:07.129520Z"
    },
    "papermill": {
     "duration": 0.014097,
     "end_time": "2022-06-30T14:10:07.132293",
     "exception": false,
     "start_time": "2022-06-30T14:10:07.118196",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_class_names(df):\n",
    "    labels = df['class']\n",
    "    return labels.unique()\n",
    "\n",
    "def make_test_augmenter(conf):\n",
    "    crop_size = round(conf.image_size*conf.crop_size)\n",
    "    return  A.Compose([\n",
    "        A.CenterCrop(height=crop_size, width=crop_size),\n",
    "        ToTensorV2(transpose_mask=True)\n",
    "    ])\n",
    "\n",
    "def get_id(filename):\n",
    "    # e.g. filename: case123_day20/scans/slice_0001_266_266_1.50_1.50.png\n",
    "    # id: case123_day20_slice_0001\n",
    "    tokens = filename.split('/')\n",
    "    return tokens[-3] + '_' + '_'.join(tokens[-1].split('_')[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca49ce34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-30T14:10:07.141854Z",
     "iopub.status.busy": "2022-06-30T14:10:07.141592Z",
     "iopub.status.idle": "2022-06-30T14:10:07.156810Z",
     "shell.execute_reply": "2022-06-30T14:10:07.155997Z"
    },
    "papermill": {
     "duration": 0.02222,
     "end_time": "2022-06-30T14:10:07.158626",
     "exception": false,
     "start_time": "2022-06-30T14:10:07.136406",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "class VisionDataset(data.Dataset):\n",
    "    def __init__(\n",
    "            self, df, conf, input_dir, imgs_dir,\n",
    "            class_names, transform, is_test=False, subset=100):\n",
    "        self.conf = conf\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "\n",
    "        if subset != 100:\n",
    "            assert subset < 100\n",
    "            # train and validate on subsets\n",
    "            num_rows = df.shape[0]*subset//100\n",
    "            df = df.iloc[:num_rows]\n",
    "\n",
    "        files = df['img_files']\n",
    "        self.files = [os.path.join(input_dir, imgs_dir, f) for f in files]\n",
    "        self.masks = [f.replace('train', 'masks') for f in files]\n",
    "\n",
    "    def resize(self, img, interp):\n",
    "        return  cv2.resize(\n",
    "            img, (self.conf.image_size, self.conf.image_size), interpolation=interp)\n",
    "\n",
    "    def load_slice(self, img_file, diff):\n",
    "        slice_num = os.path.basename(img_file).split('_')[1]\n",
    "        filename = (\n",
    "            img_file.replace(\n",
    "                'slice_' + slice_num,\n",
    "                'slice_' + str(int(slice_num) + diff).zfill(4)))\n",
    "        if os.path.exists(filename):\n",
    "            return cv2.imread(filename, cv2.IMREAD_UNCHANGED)\n",
    "        return None\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        conf = self.conf\n",
    "        img_file = self.files[index]\n",
    "        # read 5 slices into one image\n",
    "        imgs = [self.load_slice(img_file, i) for i in range(-2, 3)]\n",
    "        if imgs[3] is None:\n",
    "            imgs[3] = imgs[2]\n",
    "        if imgs[4] is None:\n",
    "            imgs[4] = imgs[3]\n",
    "        if imgs[1] is None:\n",
    "            imgs[1] = imgs[2]\n",
    "        if imgs[0] is None:\n",
    "            imgs[0] = imgs[1]\n",
    "        img = np.stack(imgs, axis=2)\n",
    "\n",
    "        img = img.astype(np.float32)\n",
    "        max_val = img.max()\n",
    "        if max_val != 0:\n",
    "            img /= max_val\n",
    "        img = self.resize(img, cv2.INTER_AREA)\n",
    "\n",
    "        if self.is_test:\n",
    "            msk = 0\n",
    "            result = self.transform(image=img)\n",
    "            img = result['image']\n",
    "        else:\n",
    "            # read mask\n",
    "            msk_file = self.masks[index]\n",
    "            msk = cv2.imread(msk_file, cv2.IMREAD_UNCHANGED)\n",
    "            msk = self.resize(msk, cv2.INTER_NEAREST)\n",
    "            msk = msk.astype(np.float32)\n",
    "            result = self.transform(image=img, mask=msk)\n",
    "            img, msk = result['image'], result['mask']\n",
    "        return img, msk\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4005cc9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-30T14:10:07.167876Z",
     "iopub.status.busy": "2022-06-30T14:10:07.167612Z",
     "iopub.status.idle": "2022-06-30T14:10:07.175249Z",
     "shell.execute_reply": "2022-06-30T14:10:07.174596Z"
    },
    "papermill": {
     "duration": 0.01408,
     "end_time": "2022-06-30T14:10:07.176856",
     "exception": false,
     "start_time": "2022-06-30T14:10:07.162776",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "class ModelWrapper(nn.Module):\n",
    "\n",
    "    def __init__(self, conf, num_classes):\n",
    "        super().__init__()\n",
    "        if conf.arch == 'FPN':\n",
    "            arch = smp.FPN\n",
    "        elif conf.arch == 'Unet':\n",
    "            arch = smp.Unet\n",
    "        elif conf.arch == 'DeepLabV3':\n",
    "            arch = smp.DeepLabV3\n",
    "        else:\n",
    "            assert 0, f'Unknown architecture {conf.arch}'\n",
    "\n",
    "        weights = 'imagenet' if conf.pretrained else None\n",
    "        self.model = arch(\n",
    "            encoder_name=conf.backbone, encoder_weights=weights, in_channels=5,\n",
    "            classes=num_classes, activation=None)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return  x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59a4b927",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-30T14:10:07.186018Z",
     "iopub.status.busy": "2022-06-30T14:10:07.185774Z",
     "iopub.status.idle": "2022-06-30T14:10:07.243912Z",
     "shell.execute_reply": "2022-06-30T14:10:07.243218Z"
    },
    "papermill": {
     "duration": 0.06456,
     "end_time": "2022-06-30T14:10:07.245500",
     "exception": false,
     "start_time": "2022-06-30T14:10:07.180940",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Config():\n",
    "    # FPN, Unet or DeepLab\n",
    "    arch = 'FPN'\n",
    "    backbone = 'timm-efficientnet-b8'\n",
    "    pretrained = True\n",
    "\n",
    "    # resize images to this size on the fly\n",
    "    image_size = 768\n",
    "    # crop to this fraction of image_size\n",
    "    crop_size = 1.0\n",
    "\n",
    "    # optimizer settings\n",
    "    optim = 'adam'\n",
    "    lr = 0.001\n",
    "    weight_decay = 0.01\n",
    "    batch_size = 1\n",
    "\n",
    "    # scheduler settings\n",
    "    gamma = 0.96\n",
    "\n",
    "    # data augmentation\n",
    "    aug_prob = 0.4\n",
    "    strong_aug = True\n",
    "    max_cutout = 0\n",
    "conf = Config()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fce0936",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-30T14:10:07.255409Z",
     "iopub.status.busy": "2022-06-30T14:10:07.255155Z",
     "iopub.status.idle": "2022-06-30T14:10:07.264025Z",
     "shell.execute_reply": "2022-06-30T14:10:07.263333Z"
    },
    "papermill": {
     "duration": 0.015855,
     "end_time": "2022-06-30T14:10:07.265662",
     "exception": false,
     "start_time": "2022-06-30T14:10:07.249807",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_test_loader(conf, input_dir, class_names):\n",
    "    test_aug = make_test_augmenter(conf)\n",
    "    test_df = pd.DataFrame()\n",
    "    img_files = []\n",
    "    img_dir = 'test'\n",
    "    subdir = ''\n",
    "    while len(img_files) == 0 and len(subdir) < 10:\n",
    "        img_files = sorted(glob(f'{input_dir}/{img_dir}/{subdir}*.png'))\n",
    "        subdir += '*/'\n",
    "        if len(subdir) > 10:\n",
    "            return None\n",
    "    # delete common prefix from paths\n",
    "    if len(img_files) == 0:\n",
    "        img_dir = 'train'\n",
    "        subdir = ''\n",
    "        while len(img_files) == 0 and len(subdir) < 10:\n",
    "            img_files = sorted(glob(f'{input_dir}/{img_dir}/{subdir}*.png'))\n",
    "            subdir += '*/'\n",
    "            if len(subdir) > 10:\n",
    "                return None      \n",
    "        img_files = img_files[:1000]\n",
    "    img_files = [f.replace(f'{input_dir}/{img_dir}/', '') for f in img_files]\n",
    "\n",
    "    test_df['img_files'] = img_files\n",
    "    test_dataset = VisionDataset(\n",
    "        test_df, conf, input_dir, img_dir,\n",
    "        class_names, test_aug, is_test=True)\n",
    "    print(f'{len(test_dataset)} examples in test set')\n",
    "    loader = data.DataLoader(\n",
    "        test_dataset, batch_size=conf.batch_size, shuffle=False,\n",
    "        num_workers=2, pin_memory=False)\n",
    "    return loader, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f50cd1a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-30T14:10:07.275169Z",
     "iopub.status.busy": "2022-06-30T14:10:07.274577Z",
     "iopub.status.idle": "2022-06-30T14:10:07.280183Z",
     "shell.execute_reply": "2022-06-30T14:10:07.279499Z"
    },
    "papermill": {
     "duration": 0.011979,
     "end_time": "2022-06-30T14:10:07.281757",
     "exception": false,
     "start_time": "2022-06-30T14:10:07.269778",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def create_model(conf, model_dir, num_classes):\n",
    "#     checkpoint = torch.load(f'{model_dir}', map_location=device)\n",
    "#     conf.pretrained = False\n",
    "#     model = ModelWrapper(conf, num_classes)\n",
    "#     model = model.to(device)\n",
    "#     model.load_state_dict(checkpoint)\n",
    "#     return model\n",
    "def create_model(conf, model_dir, num_classes):\n",
    "    checkpoint = torch.load(f'{model_dir}', map_location=device)\n",
    "    pretrained_dict = {k.replace('module.', '') : v for k, v in checkpoint.items()}\n",
    "    conf.pretrained = False\n",
    "    model = ModelWrapper(conf, num_classes)\n",
    "    model = model.to(device)\n",
    "    model.load_state_dict(pretrained_dict)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd1ff1e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-30T14:10:07.291364Z",
     "iopub.status.busy": "2022-06-30T14:10:07.290779Z",
     "iopub.status.idle": "2022-06-30T14:10:07.299761Z",
     "shell.execute_reply": "2022-06-30T14:10:07.299062Z"
    },
    "papermill": {
     "duration": 0.015515,
     "end_time": "2022-06-30T14:10:07.301400",
     "exception": false,
     "start_time": "2022-06-30T14:10:07.285885",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rle_encode(img):\n",
    "    '''\n",
    "    this function is adapted from\n",
    "    https://www.kaggle.com/code/stainsby/fast-tested-rle/notebook\n",
    "    img: numpy array, 1 - mask, 0 - background\n",
    "    Returns run length as string formated\n",
    "    '''\n",
    "    pixels = img.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "def get_img_shape(filename):\n",
    "    basename = os.path.basename(filename)\n",
    "    tokens = basename.split('_')\n",
    "    height, width = int(tokens[3]), int(tokens[2])\n",
    "    return (height, width)\n",
    "\n",
    "def pad_mask(conf, mask):\n",
    "    # pad image to conf.image_size\n",
    "    padded = np.zeros((conf.image_size, conf.image_size), dtype=mask.dtype)\n",
    "    dh = conf.image_size - mask.shape[0]\n",
    "    dw = conf.image_size - mask.shape[1]\n",
    "\n",
    "    top = dh//2\n",
    "    left = dw//2\n",
    "    padded[top:top + mask.shape[0], left:left + mask.shape[1]] = mask\n",
    "    return padded\n",
    "\n",
    "def resize_mask(mask, height, width):\n",
    "    return cv2.resize(mask, (width, height), interpolation=cv2.INTER_NEAREST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bffde92c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-30T14:10:07.310769Z",
     "iopub.status.busy": "2022-06-30T14:10:07.310447Z",
     "iopub.status.idle": "2022-06-30T14:10:07.328307Z",
     "shell.execute_reply": "2022-06-30T14:10:07.327456Z"
    },
    "papermill": {
     "duration": 0.024697,
     "end_time": "2022-06-30T14:10:07.330145",
     "exception": false,
     "start_time": "2022-06-30T14:10:07.305448",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import PIL\n",
    "import torchvision\n",
    "import torchvision.transforms.functional as F\n",
    "def run(input_dir, model_dir, thresh):\n",
    "    meta_file = os.path.join(input_dir, 'train.csv')\n",
    "    train_df = pd.read_csv(meta_file, dtype=str)\n",
    "    class_names = np.array(get_class_names(train_df))\n",
    "    num_classes = len(class_names)\n",
    "\n",
    "    model = create_model(conf, model_dir, num_classes)\n",
    "    loader, df = create_test_loader(conf, input_dir, class_names)\n",
    "    img_files = df['img_files']\n",
    "\n",
    "    subm = pd.read_csv(f'{input_dir}/sample_submission.csv')\n",
    "    del subm['predicted']\n",
    "\n",
    "    ids = []\n",
    "    classes = []\n",
    "    masks = []\n",
    "    img_idx = 0\n",
    "    sigmoid = nn.Sigmoid()\n",
    "    model.eval()\n",
    "    test_image=None\n",
    "    \n",
    "    \n",
    "#     H=W=conf.image_size\n",
    "#     shift_limit=[-0.0625,0.0625]\n",
    "#     # ratio=random.uniform(self.scale_limit[0], self.scale_limit[1])\\\n",
    "\n",
    "#     x_bias_list=[round(H*shift_limit_x) for shift_limit_x in shift_limit]\n",
    "#     y_bias_list=[round(W*shift_limit_y) for shift_limit_y in shift_limit]\n",
    "\n",
    "#     A.ShiftScaleRotate(\n",
    "#         shift_limit=0.0625, scale_limit=0.2, rotate_limit=25,\n",
    "#         interpolation=cv2.INTER_AREA, p=p)\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, _ in loader:\n",
    "            images = images.to(device)\n",
    "            test_image=images\n",
    "            size = images.size()\n",
    "            masks_tta = np.zeros((size[0], 3, size[2], size[3]), dtype=np.float32)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            preds = sigmoid(outputs).cpu().numpy()\n",
    "            masks_tta += preds\n",
    "            \n",
    "            flips = [[-1]]\n",
    "            for f in flips:\n",
    "                images_f = torch.flip(images, f)\n",
    "                outputs = model(images_f)\n",
    "                outputs = torch.flip(outputs, f)\n",
    "                preds = sigmoid(outputs).cpu().numpy()\n",
    "                masks_tta += preds\n",
    "            for degree in [25, -25]:\n",
    "                images_d = torchvision.transforms.RandomRotation(degrees=(degree, degree), expand=False, center=(size[2]//2, size[3]//2))(images)\n",
    "                outputs = model(images_d)\n",
    "                outputs = torchvision.transforms.RandomRotation(degrees=(-degree, -degree), expand=False, center=(size[2]//2, size[3]//2))(outputs)\n",
    "                preds = sigmoid(outputs).cpu().numpy()\n",
    "                masks_tta += preds\n",
    "                \n",
    "#             img=np.array(images[0].cpu()).transpose(1,2,0)\n",
    "            \n",
    "#             for i in range(2):\n",
    "#                 x_bias=random.sample(x_bias_list,1)[0]\n",
    "#                 y_bias=random.sample(y_bias_list,1)[0]\n",
    "#                 tta_fun= A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=25,interpolation=cv2.INTER_AREA, p=1)\n",
    "#                 images_s=shift(images,x_bias,y_bias,'input')\n",
    "#                 outputs = model(images_s)\n",
    "#                 outputs =shift(outputs,-x_bias,-y_bias)\n",
    "#                 preds = sigmoid(outputs).cpu().numpy()\n",
    "#                 masks_tta += preds\n",
    "                \n",
    "            masks_tta /= 6\n",
    "            \n",
    "            masks_tta[masks_tta >= thresh] = 1\n",
    "            masks_tta[masks_tta < thresh] = 0\n",
    "            for pred in masks_tta:\n",
    "                img_file = img_files[img_idx]\n",
    "                img_idx += 1\n",
    "                img_id = get_id(img_file)\n",
    "                height, width = get_img_shape(img_file)\n",
    "                for class_id, class_name in enumerate(class_names):\n",
    "                    mask = pred[class_id]\n",
    "                    mask = pad_mask(conf, mask)\n",
    "                    mask = resize_mask(mask, height, width)\n",
    "                    enc_mask = '' if mask.sum() == 0 else rle_encode(mask)\n",
    "                    ids.append(img_id)\n",
    "                    classes.append(class_name)\n",
    "                    masks.append(enc_mask)\n",
    "\n",
    "    pred_df = pd.DataFrame({'id': ids, 'class': classes, 'predicted': masks})\n",
    "    if pred_df.shape[0] > 0:\n",
    "        # sort according to the given order and save to a csv file\n",
    "        subm = subm.merge(pred_df, on=['id', 'class'])\n",
    "    subm.to_csv('submission.csv', index=False)\n",
    "    return test_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40799a89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-30T14:10:07.340021Z",
     "iopub.status.busy": "2022-06-30T14:10:07.339467Z",
     "iopub.status.idle": "2022-06-30T14:18:10.744921Z",
     "shell.execute_reply": "2022-06-30T14:18:10.743970Z"
    },
    "papermill": {
     "duration": 483.417308,
     "end_time": "2022-06-30T14:18:10.751826",
     "exception": false,
     "start_time": "2022-06-30T14:10:07.334518",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 examples in test set\n",
      "over\n"
     ]
    }
   ],
   "source": [
    "test_thresh = 0.4\n",
    "model_path='../input/checkpoint/model_2.pth'\n",
    "test_image=run('../input/uw-madison-gi-tract-image-segmentation', model_path, test_thresh)\n",
    "print('over')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b62da912",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-30T14:18:10.763176Z",
     "iopub.status.busy": "2022-06-30T14:18:10.762446Z",
     "iopub.status.idle": "2022-06-30T14:18:10.767099Z",
     "shell.execute_reply": "2022-06-30T14:18:10.766376Z"
    },
    "papermill": {
     "duration": 0.011531,
     "end_time": "2022-06-30T14:18:10.768678",
     "exception": false,
     "start_time": "2022-06-30T14:18:10.757147",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test_image_save=test_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6549428",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-30T14:18:10.778167Z",
     "iopub.status.busy": "2022-06-30T14:18:10.777912Z",
     "iopub.status.idle": "2022-06-30T14:18:10.781325Z",
     "shell.execute_reply": "2022-06-30T14:18:10.780486Z"
    },
    "papermill": {
     "duration": 0.010051,
     "end_time": "2022-06-30T14:18:10.782943",
     "exception": false,
     "start_time": "2022-06-30T14:18:10.772892",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test_image_save.sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 623.560446,
   "end_time": "2022-06-30T14:18:13.660660",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-06-30T14:07:50.100214",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
